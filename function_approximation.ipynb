{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dealer_intervals = np.array([[1, 4], [4, 7], [7, 10]])\n",
    "player_intervals = np.array([[1, 6], [4, 9], [7, 12], [10, 15], [13, 18], [16, 21]])\n",
    "actions = np.array([0, 1])\n",
    "\n",
    "\n",
    "def initialize_weights_or_features(value=0):\n",
    "  return np.full((dealer_intervals.shape[0], player_intervals.shape[0], actions.shape[0]), value, dtype=float)\n",
    "\n",
    "\n",
    "def find_intervals(sum_value, intervals):\n",
    "    found_intervals = []\n",
    "\n",
    "    for i in range(len(intervals)):\n",
    "        if intervals[i][0] <= sum_value <= intervals[i][1]:\n",
    "            found_intervals.append(i)\n",
    "\n",
    "    return found_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_Q(state, action, weights, features):\n",
    "  dealer_sum, player_sum = state\n",
    "  found_dealer_intervals = find_intervals(dealer_sum, dealer_intervals)\n",
    "  found_player_intervals = find_intervals(player_sum, player_intervals)\n",
    "\n",
    "  value = 0\n",
    "  for i in found_dealer_intervals:\n",
    "    for j in found_player_intervals:\n",
    "      value += weights[i][j][action] * features[i][j][action]\n",
    "\n",
    "  return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, weights, features):\n",
    "  r = random.random()\n",
    "  if r < 0.05:\n",
    "    return random.choice([action.value for action in Action])\n",
    "  q0 = approx_Q(state, 0, weights, features)\n",
    "  q1 = approx_Q(state, 1, weights, features)\n",
    "  return 0 if q0 > q1 else 1  # could choose randomly here if they are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(state, action):\n",
    "  dealer_sum, player_sum = state\n",
    "  found_dealer_intervals = find_intervals(dealer_sum, dealer_intervals)\n",
    "  found_player_intervals = find_intervals(player_sum, player_intervals)\n",
    "\n",
    "  features = np.zeros((3, 6, 2), dtype=int)\n",
    "\n",
    "  for i in found_dealer_intervals:\n",
    "    for j in found_player_intervals:\n",
    "      features[i, j, action] = 1\n",
    "\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_lambda(env: Env, _lambda=0, gamma=1,num_episodes=1000, N_0=100, show_progress=True):\n",
    "  features = initialize_weights_or_features()\n",
    "  weights = initialize_weights_or_features()\n",
    "  N_s = {}\n",
    "  N_sa = {}\n",
    "  mse = []\n",
    "  wins = 0\n",
    "  alpha = 0.01\n",
    "\n",
    "  for i in range(num_episodes):\n",
    "    e = initialize_weights_or_features()\n",
    "    dealer_sum = NewCard(firstCard=True).get_value()\n",
    "    player_sum = NewCard(firstCard=True).get_value()\n",
    "    terminated = False\n",
    "    state = (dealer_sum, player_sum)\n",
    "    action = int(epsilon_greedy(state, weights, features))    \n",
    "    current_Q = approx_Q(state, action, weights, features) \n",
    "\n",
    "    while not terminated:\n",
    "      N_s[state] = N_s.get(state, 0) + 1\n",
    "      N_sa[(state, action)] = N_sa.get((state, action), 0) + 1\n",
    "\n",
    "      dealer_sum, player_sum, reward, terminated = env.step(\n",
    "        dealer_sum, player_sum, action\n",
    "      )\n",
    "      wins += reward == 1\n",
    "      \n",
    "      new_state = (dealer_sum, player_sum)\n",
    "      new_action = epsilon_greedy(state, weights, features)\n",
    "      if terminated:\n",
    "        delta = reward - gamma * current_Q\n",
    "      else:\n",
    "        next_Q = approx_Q(new_state, new_action, weights, features) \n",
    "        delta = reward + gamma * next_Q - current_Q\n",
    "      \n",
    "      e += phi(state, action)\n",
    "      weights += alpha * delta * e\n",
    "      weights *= _lambda * gamma\n",
    "      state = new_state\n",
    "      action = new_action\n",
    "\n",
    "    if not show_progress:\n",
    "      continue\n",
    "\n",
    "    if i % (num_episodes / 10) == 0 and i > 0:\n",
    "      print(\"Episode: %d, score: %f\" % (i, (float(wins) / i * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, score: 52.300000\n",
      "Episode: 2000, score: 53.200000\n",
      "Episode: 3000, score: 53.466667\n",
      "Episode: 4000, score: 53.250000\n",
      "Episode: 5000, score: 52.960000\n",
      "Episode: 6000, score: 52.400000\n",
      "Episode: 7000, score: 52.414286\n",
      "Episode: 8000, score: 52.500000\n",
      "Episode: 9000, score: 52.633333\n"
     ]
    }
   ],
   "source": [
    "sarsa_lambda(env, _lambda=0.5, num_episodes=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
